{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yz4C-rzVuXeU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchsummary\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data in json format\n",
        "\n",
        "os.makedirs(\"images_train\", exist_ok=True)\n",
        "os.makedirs(\"images_test\", exist_ok=True)\n",
        "\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Create JSON files for training and test datasets\n",
        "def create_json(dataset, json_filename):\n",
        "    data_list = []\n",
        "    for i, (image, label) in enumerate(dataset):\n",
        "        data_list.append({\"img_path\": f\"{i}.png\", \"labels\": label})\n",
        "        base_name = os.path.basename(json_filename)\n",
        "        type_data = base_name.split('_')[0] # returns train or test\n",
        "        directory = \"images_\" + type_data\n",
        "        image_path = os.path.join(directory, f\"{i}.png\")\n",
        "        torchvision.utils.save_image(image, image_path)\n",
        "    with open(json_filename, 'w') as json_file:\n",
        "      for data in data_list:\n",
        "        json.dump(data, json_file)\n",
        "        json_file.write('\\n')\n",
        "\n",
        "create_json(train_data, \"train_data.json\")\n",
        "create_json(test_data, \"test_data.json\")"
      ],
      "metadata": {
        "id": "ZvZShlYQ4s6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2ac58d-e744-491d-fc27-54e19290074e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16555115.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 299659.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5558579.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 15015491.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset customed\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, json_filename, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            json_filename (string): Json file with images paths and labels.\n",
        "            transform (callable, optional): Optional transform to be applied.\n",
        "        \"\"\"\n",
        "        self.data_list = []\n",
        "        with open(json_filename, 'r') as file:\n",
        "          for line in file:\n",
        "              loaded_dict = json.loads(line)\n",
        "              self.data_list.append(loaded_dict)\n",
        "        self.transform = transform\n",
        "        self.json_filename = json_filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_path_aux = self.data_list[idx]['img_path']\n",
        "      base_name = os.path.basename(self.json_filename)\n",
        "      type_data = base_name.split('_')[0]\n",
        "      directory = \"images_\" + type_data\n",
        "      img_path = os.path.join(directory, img_path_aux)\n",
        "      image = Image.open(img_path)\n",
        "      if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "      label = self.data_list[idx]['labels']\n",
        "      return image, label\n",
        "\n",
        "\n",
        "# Transforming the image in getitem\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    #transforms.RandomCrop((224,224))\n",
        "    transforms.Grayscale(num_output_channels=1), # 28x28x1\n",
        "    transforms.ToTensor()\n",
        "    #transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dataset = CustomImageDataset(\"train_data.json\", transform)\n",
        "test_dataset = CustomImageDataset(\"test_data.json\", transform)"
      ],
      "metadata": {
        "id": "DQduwX-hvknz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code divides the training in train-val data\n",
        "\n",
        "# 70 % of 60.000 training and 30% validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "print(\"Train size: \", train_size, \"\\nValidation size: \", val_size, \"\\nTest size: \", len(test_dataset))\n",
        "new_train_dataset, new_val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(new_val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsKc7YVV96ld",
        "outputId": "f1dc0c13-5e9a-4729-b101-93cb487494b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size:  48000 \n",
            "Validation size:  12000 \n",
            "Test size:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BASELINE: LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            # convolution: 6 out channels/filters = 6@28x28, a convolution of 5x5 is applied -> kernel_size 5\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),   # padding 2 because input images are 28x28 (not 32x32)\n",
        "            nn.ReLU(),\n",
        "            # average pooling\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  # 14*14\n",
        "            # convolution: it recieves 6 from previous convolution and now 16@10x10\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=16*5*5, out_features=120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=84, out_features=10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.classifier(self.feature(x))\n",
        "\n",
        "\n",
        "# Kaiming Uniform initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # in Alex computer no cuda, already checked, Alimo you could check\n",
        "\n",
        "baseline_model = LeNet5().to(device)\n",
        "baseline_model.apply(init_weights)\n",
        "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "torchsummary.summary(baseline_model, input_size=(1,28 , 28))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3uH41yx6mCK",
        "outputId": "259ce0fa-2a58-4bb1-c57a-96902ced170a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "              ReLU-2            [-1, 6, 28, 28]               0\n",
            "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
            "              ReLU-5           [-1, 16, 10, 10]               0\n",
            "         AvgPool2d-6             [-1, 16, 5, 5]               0\n",
            "           Flatten-7                  [-1, 400]               0\n",
            "            Linear-8                  [-1, 120]          48,120\n",
            "              ReLU-9                  [-1, 120]               0\n",
            "           Linear-10                   [-1, 84]          10,164\n",
            "             ReLU-11                   [-1, 84]               0\n",
            "           Linear-12                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.11\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative 1: Using Dropout to see less overfiting\n",
        "class CNN1(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.feature = nn.Sequential(\n",
        "            # convolution: 6 out channels/filters = 6@28x28, a convolution of 5x5 is applied -> kernel_size 5\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),   # padding 2 because input images are 28x28 (not 32x32)\n",
        "            nn.ReLU(),\n",
        "            # average pooling\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  # 14*14\n",
        "            # convolution: it recieves 6 from previous convolution and now 16@10x10\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "      self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=16*5*5, out_features=120),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=84, out_features=10),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "      return self.classifier(self.feature(x))\n",
        "\n",
        "\n",
        "cnn1 = CNN1().to(device)\n",
        "cnn1.apply(init_weights)\n",
        "optimizer = optim.Adam(cnn1.parameters(), lr=0.001)\n",
        "\n",
        "torchsummary.summary(cnn1, input_size=(1,28 , 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl5uJoc09PkK",
        "outputId": "bac7b9b6-8b6b-4bc4-dd63-839463407f75"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "              ReLU-2            [-1, 6, 28, 28]               0\n",
            "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
            "              ReLU-5           [-1, 16, 10, 10]               0\n",
            "         AvgPool2d-6             [-1, 16, 5, 5]               0\n",
            "           Flatten-7                  [-1, 400]               0\n",
            "            Linear-8                  [-1, 120]          48,120\n",
            "              ReLU-9                  [-1, 120]               0\n",
            "          Dropout-10                  [-1, 120]               0\n",
            "           Linear-11                   [-1, 84]          10,164\n",
            "             ReLU-12                   [-1, 84]               0\n",
            "          Dropout-13                   [-1, 84]               0\n",
            "           Linear-14                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.12\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  running_loss = 0\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #print(images.shape)\n",
        "      # Forward Pass\n",
        "      output = model(images)\n",
        "      loss = loss_fn(output, labels)\n",
        "\n",
        "      # Backward Pass\n",
        "      loss.backward()\n",
        "      optimizer.step() # updates model parameters using gradient computings by back propagation and applies the optimization algo\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if batch_idx % 100 == 99:\n",
        "          print(f'Batch {batch_idx+1}/{len(train_loader)}, Loss: {running_loss/100:.4f}')\n",
        "          running_loss = 0.0\n",
        "\n",
        "def test(data_loader, model, loss_fn, type_test):\n",
        "  with torch.no_grad():\n",
        "      correct, total , loss = 0, 0, 0\n",
        "      num_batches = len(data_loader)\n",
        "      model.eval()\n",
        "      for images, labels in data_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          loss += loss_fn(labels.long(), outputs.float()).item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  loss /= num_batches\n",
        "\n",
        "  print(f\"{type_test} accuracy: {100 * correct / total} %, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "9-A2iQ9lCZfy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run (models, epochs = 5):\n",
        "  training_results = []\n",
        "  validation_results = []\n",
        "  for model in models:\n",
        "    print(\"Model: \", model)\n",
        "    for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n---------------------\")\n",
        "      training_accuracy, training_loss = train(baseline_model, device, train_loader, optimizer, loss_fn)\n",
        "      validation_accuracy, validation_loss = test(val_loader, baseline_model, loss_fn, \"Validation\")\n",
        "      training_results.append((training_accuracy, training_loss))\n",
        "      validation_results.append((validation_accuracy, validation_loss))\n",
        "  return training_results, validation_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UEhRS70CPBi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [baseline_model, cnn1]\n",
        "\n",
        "training_results, validation_results = run(models, epochs = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RWPpWBSg_GbX",
        "outputId": "95d1e6dc-ab4a-433d-c46f-b1e98e5701a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  LeNet5(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "---------------------\n",
            "Batch 100/1500, Loss: 0.4142\n",
            "Batch 200/1500, Loss: 0.4051\n",
            "Batch 300/1500, Loss: 0.3973\n",
            "Batch 400/1500, Loss: 0.4073\n",
            "Batch 500/1500, Loss: 0.4068\n",
            "Batch 600/1500, Loss: 0.4074\n",
            "Batch 700/1500, Loss: 0.3843\n",
            "Batch 800/1500, Loss: 0.4132\n",
            "Batch 900/1500, Loss: 0.4116\n",
            "Batch 1000/1500, Loss: 0.3843\n",
            "Batch 1100/1500, Loss: 0.4022\n",
            "Batch 1200/1500, Loss: 0.3954\n",
            "Batch 1300/1500, Loss: 0.4238\n",
            "Batch 1400/1500, Loss: 0.4106\n",
            "Batch 1500/1500, Loss: 0.4301\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c6da714ebab6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-37e2221b04e3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(models, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-5f6170ea13ec>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data_loader, model, loss_fn, type_test)\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
          ]
        }
      ]
    }
  ]
}